{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn random-forest 특성 중요도 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=ConvergenceWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importances(clf, X_train, y_train=None, \n",
    "                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n",
    "    '''\n",
    "    plot feature importances of a tree-based sklearn estimator\n",
    "    \n",
    "    Note: X_train and y_train are pandas DataFrames\n",
    "    \n",
    "    Note: Scikit-plot is a lovely package but I sometimes have issues\n",
    "              1. flexibility/extendibility\n",
    "              2. complicated models/datasets\n",
    "          But for many situations Scikit-plot is the way to go\n",
    "          see https://scikit-plot.readthedocs.io/en/latest/Quickstart.html\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        clf         (sklearn estimator) if not fitted, this routine will fit it\n",
    "        \n",
    "        X_train     (pandas DataFrame)\n",
    "        \n",
    "        y_train     (pandas DataFrame)  optional\n",
    "                                        required only if clf has not already been fitted \n",
    "        \n",
    "        top_n       (int)               Plot the top_n most-important features\n",
    "                                        Default: 10\n",
    "                                        \n",
    "        figsize     ((int,int))         The physical size of the plot\n",
    "                                        Default: (8,8)\n",
    "        \n",
    "        print_table (boolean)           If True, print out the table of feature importances\n",
    "                                        Default: False\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "        the pandas dataframe with the features and their importance\n",
    "        \n",
    "    Author\n",
    "    ------\n",
    "        George Fisher\n",
    "    '''\n",
    "    \n",
    "    __name__ = \"plot_feature_importances\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy  as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    from xgboost.core     import XGBoostError\n",
    "    from lightgbm.sklearn import LightGBMError\n",
    "    \n",
    "    try: \n",
    "        if not hasattr(clf, 'feature_importances_'):\n",
    "            clf.fit(X_train.values, y_train.values.ravel())\n",
    "\n",
    "            if not hasattr(clf, 'feature_importances_'):\n",
    "                raise AttributeError(\"{} does not have feature_importances_ attribute\".\n",
    "                                    format(clf.__class__.__name__))\n",
    "                \n",
    "    except (XGBoostError, LightGBMError, ValueError):\n",
    "        clf.fit(X_train.values, y_train.values.ravel())\n",
    "            \n",
    "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
    "    feat_imp['feature'] = X_train.columns\n",
    "    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    feat_imp = feat_imp.iloc[:top_n]\n",
    "    \n",
    "    feat_imp.sort_values(by='importance', inplace=True)\n",
    "    feat_imp = feat_imp.set_index('feature', drop=True)\n",
    "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.show()\n",
    "    \n",
    "    if print_table:\n",
    "        from IPython.display import display\n",
    "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
    "        display(feat_imp.sort_values(by='importance', ascending=False))\n",
    "        \n",
    "    return feat_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost              import XGBClassifier\n",
    "from sklearn.ensemble     import ExtraTreesClassifier\n",
    "from sklearn.tree         import ExtraTreeClassifier\n",
    "from sklearn.tree         import DecisionTreeClassifier\n",
    "from sklearn.ensemble     import GradientBoostingClassifier\n",
    "from sklearn.ensemble     import BaggingClassifier\n",
    "from sklearn.ensemble     import AdaBoostClassifier\n",
    "from sklearn.ensemble     import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm             import LGBMClassifier\n",
    "\n",
    "\n",
    "\n",
    "def test_model_importance():   \n",
    "    # X, y 파싱\n",
    "    iris = datasets.load_iris()\n",
    "\n",
    "    # type: ndarray\n",
    "    X = iris.data \n",
    "    y = iris.target\n",
    "    \n",
    "    # 교차 검증\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    # 모델\n",
    "    clfs = [XGBClassifier(),              LGBMClassifier(), \n",
    "        ExtraTreesClassifier(),       ExtraTreeClassifier(),\n",
    "        BaggingClassifier(),          DecisionTreeClassifier(),\n",
    "        GradientBoostingClassifier(), LogisticRegression(),\n",
    "        AdaBoostClassifier(),         RandomForestClassifier()\n",
    "    ]\n",
    "    \n",
    "    for clf in clfs:\n",
    "        try:\n",
    "            _ = plot_feature_importances(clf, X_train, y_train, top_n=X_train.shape[1], title=clf.__class__.__name__)\n",
    "        except AttributeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n",
      "'numpy.ndarray' object has no attribute 'values'\n"
     ]
    }
   ],
   "source": [
    "test_model_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'numpy' has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-cdb8981d44ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_trin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0miris_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0miris_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 특성 4개가 있는 것을 확인\n",
    "iris = datasets.load_iris()\n",
    "print(iris.data.shape)\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_trin, X_test = train_test_split(X,y)\n",
    "\n",
    "iris_df = pd.DataFrame(X_train)\n",
    "iris_df.columns = X.features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
